<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Daiqing Li</title>
  
  <meta name="author" content="Daiqing Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Daiqing Li</name>
              </p>
              <p>I recently join <a href="https://playgroundai.com/"> Playground</a> as a Research Lead. We are working on <b>pixel foundation models</b>, and we are hiring! Send me an email if you are interested. </p>
              <p>I was a Senior Research Scientist at <a href="https://research.nvidia.com/labs/toronto-ai/">NVIDIA Toronto AI Lab</a> in Toronto, where I work on computer vision, computer graphics, generative models and machine learning.
              </p>
              <p>
                At NVIDIA, I work closely with <a href="https://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a> and <a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba</a>. 
                Several of our works have been integrated into NVIDIA products like <a href="https://www.nvidia.com/en-us/omniverse/">Omniverse</a> and <a href="https://www.nvidia.com/en-us/clara/">Clara</a>. I graduated from <a href="https://www.utoronto.ca/">University of Toronto</a> 
                and I recieved <a href="http://www.miccai.org/about-miccai/awards/best-paper-award-and-young-scientist-award/">MICCAI Young Scientist Awards </a> runner-up.
              </p>
              <p style="text-align:center">
                <a href="mailto:lidaiqing2016@gmail.com">Email</a> &nbsp/&nbsp
                <!-- <a href="">CV</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.ca/citations?user=8q2ISMIAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/lidaiqing">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/lidaiqing">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/daiqing-circle.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/daiqing-circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle;">
              <heading>News</heading>
              <p style="margin-bottom:10px;">
                <li>
                  <strong> Oct 2024:</strong> Arxiv technical report <a href="https://arxiv.org/abs/2409.10695">Playground v3</a>. A model achieves SoTA text generation and text-image consistency.
                </li>
                <li>
                  <strong> Feb 2024:</strong> Open souce <a href="https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic">Playground v2.5</a>. A model achieves better aesthetic quality than Midjourney 5.2.
                </li>
                <li>
                  <strong> Dec 2023:</strong> Gave a <a href="https://www.cvg.unibe.ch/research/talks/">talk</a> at University of Bern.
                </li>
                <li>
                  <strong> Dec 2023:</strong> Open souce <a href="https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic">Playground v2</a>. A <b>2.5x</b> better model over SDXL in user preference.
                </li>
                <li>
                  <strong> Aug 2023:</strong> I join <a href="https://playgroundai.com/"> Playground</a> as a research lead.
                </li>
                <li>
                  <strong> July 2023:</strong> DreamTeacher is accepted to ICCV 2023.
                </li>
              </p>
            </td>
          </tr>
        </tbody></table>
      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, computer graphics, generative models and machine learning. Much of my research is about exploiting generative models for various computer vision tasks, such as semantic segmentation, image editing, and representation learning.
              </p>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="pg3_stop()" onmouseover="pg3_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='pg3_image'><video  width=100% height=100% muted autoplay loop>
                <source src="" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/pgv3_from.png' width="160">
              </div>
              <script type="text/javascript">
                function pg3_start() {
                  document.getElementById('pg3_image').style.opacity = "1";
                }
      
                function pg3_stop() {
                  document.getElementById('pg3_image').style.opacity = "0";
                }
                pg3_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic">
                <papertitle>Playground v3: Improving Text-to-Image Alignment with Deep-Fusion Large Language Models</papertitle>
              </a>
              <br>
              
              <a href="">Bingchen Liu</a>,
              <a href="">Ehsan Akhgari</a>,
              <a href="">Alexander Visheratin</a>,
              <a href="">Aleks Kamko</a>, <br>
              <a href="">Linmiao Xu</a>,
              <a href="">Shivam Shrirao</a>,
              <a href="">Joao Souza</a>,
              <a href="">Suhail Doshi</a>,
              <strong>Daiqing Li</strong>
              <br>
              <em>Arxiv</em>, 2024
              <br>
              <a href="https://playground.com/pg-v3">blog</a>
              /
              <a href="https://www.youtube.com/watch?v=VPv4LCi3eWo&t=1s">video</a>
              /
              <a href="https://arxiv.org/abs/2409.10695">arXiv</a>
              <p></p>
              <p>
                We propose a new text-to-image model architecture that deep-fusion large language models(Llama3) to improve text-to-image alignment. Our model achieves state-of-the-art performance in terms of text generation and text-image consistency, better than Flux and Ideogram.
              </p>
            </td>
          </tr>
          <tr onmouseout="pg25_stop()" onmouseover="pg25_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='pg25_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/pg2.5.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/pgv2.5-from.png' width="160">
              </div>
              <script type="text/javascript">
                function pg25_start() {
                  document.getElementById('pg25_image').style.opacity = "1";
                }
      
                function pg25_stop() {
                  document.getElementById('pg25_image').style.opacity = "0";
                }
                pg25_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic">
                <papertitle>Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in Text-to-Image Generation</papertitle>
              </a>
              <br>
              <strong>Daiqing Li</strong>, 
              <a href="">Aleks Kamko</a>,
              <a href="">Ehsan Akhgari</a>,
              <a href="">Ali Sabet</a>,
              <a href="">Linmiao Xu</a>, <br>
              <a href="">Suhail Doshi</a>,
              <br>
              <em>Arxiv</em>, 2024
              <br>
              <a href="https://playground.com/blog/playground-v2-5">blog</a>
              /
              <a href="https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic">huggingface</a>
              /
              <a href="./images/pg2.5.mp4">video</a>
              /
              <a href="https://arxiv.org/abs/2402.17245">arXiv</a>
              <p></p>
              <p>
                We share three insights to enhance aesthetic quality in text-to-image generation. Our new model achieves better aesthetic quality than Midjourney 5.2 and beats SDXL with a large margin in all multi-aspect ratios conditions.
              </p>
            </td>
          </tr>
          <tr onmouseout="dreamteacher_stop()" onmouseover="dreamteacher_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dreamteacher_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/dreamteacher_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/dreamteacher_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function dreamteacher_start() {
                  document.getElementById('dreamteacher_image').style.opacity = "1";
                }
      
                function dreamteacher_stop() {
                  document.getElementById('dreamteacher_image').style.opacity = "0";
                }
                dreamteacher_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://research.nvidia.com/labs/toronto-ai/DreamTeacher/">
                <papertitle>DreamTeacher: Pretraining Image Backbones with Deep Generative Models</papertitle>
              </a>
              <br>
              <strong>Daiqing Li*</strong>, 
              <a href="http://www.cs.toronto.edu/~linghuan/">Huan Ling*</a>,
              <a href="https://amlankar.github.io/">Amlan Kar</a>,
              <a href="http://www.cs.toronto.edu/~davidj/">David Acuna</a>,
              <a href="https://seung-kim.github.io/seungkim/">Seung Wook Kim</a>, <br>
              <a href="https://karstenkreis.github.io/">Karsten Kreis</a>,
              <a href="http://web.mit.edu/torralba/www/">Antonio Torralba</a>,
              <a href="http://www.cs.toronto.edu/~fidler/">Sanja Fidler</a>,
              <br>
              <em>ICCV</em>, 2023
              <br>
              <a href="https://research.nvidia.com/labs/toronto-ai/DreamTeacher/">project page</a>
              /
              <a href="https://research.nvidia.com/labs/toronto-ai/DreamTeacher/resources/dream_teacher_seg_short.mp4">video</a>
              /
              <a href="https://arxiv.org/abs/2307.07487">arXiv</a>
              <p></p>
              <p>
                We propose a new pre-training framework by distilling knowledge from generative models onto commonly-use image backbones, and show generative models, 
                as a promising approach to representation learning on large, diverse datasets without requiring manual annotation.
              </p>
            </td>
          </tr>
          <tr onmouseout="nfldm_stop()" onmouseover="nfldm_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nfldm_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/nfldm_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/nfldm_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function nfldm_start() {
                  document.getElementById('nfldm_image').style.opacity = "1";
                }
      
                function nfldm_stop() {
                  document.getElementById('nfldm_image').style.opacity = "0";
                }
                nfldm_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://research.nvidia.com/labs/toronto-ai/NFLDM/">
                <papertitle>NeuralField-LDM: Scene Generation with Hierarchical Latent Diffusion Models</papertitle>
              </a>
              <br>
              <a href="https://seung-kim.github.io/seungkim/">Seung Wook Kim</a>,
              <a href="https://www.bradbrown.ca/">Bradley Brown</a>,
              <a href="https://kangxue.org/">Kangxue Yin</a>,
              <a href="https://karstenkreis.github.io/">Karsten Kreis</a>,
              <a href="https://katjaschwarz.github.io/">Katja Schwarz</a>, <br>
              <strong>Daiqing Li</strong>, 
              <a href="https://scholar.google.com/citations?user=ygdQhrIAAAAJ&hl=en">Robin Rombach</a>,
              <a href="http://web.mit.edu/torralba/www/">Antonio Torralba</a>,
              <a href="http://www.cs.toronto.edu/~fidler/">Sanja Fidler</a>,
              <br>
              <em>CVPR</em>, 2023
              <br>
              <a href="https://research.nvidia.com/labs/toronto-ai/NFLDM/">project page</a>
              /
              <a href="https://research.nvidia.com/labs/toronto-ai/NFLDM/assets/samples_.mp4">video</a>
              /
              <a href="https://arxiv.org/abs/2304.09787">arXiv</a>
              <p></p>
              <p>
                We use lift-splat-shoot like representation to encode driving scene and nerf like representation to decode scenes with view controls. We then learn a hierarchical LDM on the latent representation for driving scene generations.
              </p>
            </td>
          </tr>
          <tr onmouseout="get3d_stop()" onmouseover="get3d_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='get3d_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/get3d_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/get3d_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function get3d_start() {
                  document.getElementById('get3d_image').style.opacity = "1";
                }
      
                function get3d_stop() {
                  document.getElementById('get3d_image').style.opacity = "0";
                }
                get3d_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://nv-tlabs.github.io/GET3D/">
                <papertitle>GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images</papertitle>
              </a>
              <br>
              <a href="http://www.cs.toronto.edu/~jungao/">Jun Gao</a>,
              <a href="http://www.cs.toronto.edu/~shenti11/">Tianchang Shen</a>,
              <a href="http://www.cs.toronto.edu/~zianwang/">Zian Wang</a>,
              <a href="http://www.cs.toronto.edu/~wenzheng/">Wenzheng Chen</a>, <br>
              <a href="https://kangxue.org/">Kangxue Yin</a>,
              <strong>Daiqing Li</strong>,
              <a href="https://orlitany.github.io/">Or Litany</a>,
              <a href="https://zgojcic.github.io/">Zan Gojcic</a>,
              <a href="http://www.cs.toronto.edu/~fidler/">Sanja Fidler</a>,
              <br>
              <em>NeurIPS</em>, 2022 &nbsp <font color="red"><strong>(Spotlight Presentation)</strong></font>
              <br>
              <a href="https://nv-tlabs.github.io/GET3D/">project page</a>
              /
              <a href="https://www.youtube.com/watch?v=shy51E-MU8Y">video</a>
              /
              <a href="https://arxiv.org/abs/2209.11163">arXiv</a>
              <p></p>
              <p>
                We develop a 3D generative model to generate meshes with textures, bridging the success in the differentiable surface modeling, differentiable rendering and 2D GANs.
              </p>
            </td>
          </tr>
          <tr onmouseout="esdata_stop()" onmouseover="esdata_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='esdata_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/esdata_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/esdata_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function esdata_start() {
                  document.getElementById('esdata_image').style.opacity = "1";
                }
      
                function esdata_stop() {
                  document.getElementById('esdata_image').style.opacity = "0";
                }
                esdata_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://research.nvidia.com/labs/toronto-ai/estimatingrequirements/">
                <papertitle>How Much More Data Do I Need? Estimating Requirements for Downstream Tasks</papertitle>
              </a>
              <br>
              <a href="https://rafidrm.github.io/">Rafid Mahmood</a>,
              <a href="https://www.cs.toronto.edu/~jlucas/">James Lucas</a>,
              <a href="https://cs.toronto.edu/~davidj/">David Acuna</a>,
              <strong>Daiqing Li</strong>,
              <a href="https://cs.toronto.edu/~jphilion/">Jonah Philion</a>, <br>
              <a href="https://alvarezlopezjosem.github.io/">Jose M. Alvarez</a>,
              <a href="https://chrisding.github.io/">Zhiding Yu</a>,
              <a href="http://www.cs.toronto.edu/~fidler/">Sanja Fidler</a>,
              <a href="https://www.cs.utoronto.edu/~law/">Marc T. Law</a>,
              <br>
              <em>CVPR</em>, 2022
              <br>
              <a href="https://research.nvidia.com/labs/toronto-ai/estimatingrequirements/">project page</a>
              /
              <a href="https://research.nvidia.com/labs/toronto-ai/estimatingrequirements/assets/TwitterPreviewSlides.mp4">video</a>
              /
              <a href="https://arxiv.org/abs/2207.01725">arXiv</a>
              <p></p>
              <p>
                We use a family of functions that generalize the power-law to allow for better estimation of data requirements under limited budgets.
              </p>
            </td>
          </tr>
          <tr onmouseout="polygan_stop()" onmouseover="polygan_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='polygan_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/polygan_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/polygan_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function polygan_start() {
                  document.getElementById('polygan_image').style.opacity = "1";
                }
      
                function polygan_stop() {
                  document.getElementById('polygan_image').style.opacity = "0";
                }
                polygan_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://nv-tlabs.github.io/PMGAN/">
                <papertitle>Polymorphic-GAN: Generating Aligned Samples across Multiple Domains with Learned Morph Maps</papertitle>
              </a>
              <br>
              <a href="https://seung-kim.github.io/seungkim/">Seung Wook Kim</a>,
              <a href="https://karstenkreis.github.io/">Karsten Kreis</a>,
              <strong>Daiqing Li</strong>, <br>
              <a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba</a>,
              <a href="http://www.cs.toronto.edu/~fidler/">Sanja Fidler</a>,
              <br>
              <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://nv-tlabs.github.io/PMGAN/">project page</a>
              /
              <a href="https://nv-tlabs.github.io/PMGAN/assets/overview_compressed.mp4">video</a>
              /
              <a href="https://arxiv.org/abs/2206.02903">arXiv</a>
              <p></p>
              <p>
                We use GAN to model multi-domain objects with shared attributes, and use morph net to model geometry differences. We show its application in segmentation transfer and image editting tasks.
              </p>
            </td>
          </tr>
          <tr onmouseout="bigdatasetgan_stop()" onmouseover="bigdatasetgan_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='bigdatasetgan_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/bigdatasetgan_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/bigdatasetgan_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function bigdatasetgan_start() {
                  document.getElementById('editgan_image').style.opacity = "1";
                }
      
                function bigdatasetgan_stop() {
                  document.getElementById('editgan_image').style.opacity = "0";
                }
                bigdatasetgan_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://nv-tlabs.github.io/big-datasetgan/">
                <papertitle>BigDatasetGAN: Synthesizing ImageNet with Pixel-wise Annotations</papertitle>
              </a>
              <br>
              <strong>Daiqing Li</strong>,
              <a href="http://www.cs.toronto.edu/~linghuan/">Huan Ling</a>,
              <a href="https://seung-kim.github.io/seungkim/">Seung Wook Kim</a>,
              <a href="https://karstenkreis.github.io/">Karsten Kreis</a>, <br>
              <a href="">Adela Barriuso</a>,
              <a href="http://www.cs.toronto.edu/~fidler/">Sanja Fidler</a>,
              <a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba</a>
              <br>
              <em>CVPR</em>, 2022
              <br>
              <a href="https://nv-tlabs.github.io/big-datasetgan/">project page</a>
              /
              <a href="https://nv-tlabs.github.io/big-datasetgan/resources/cvpr22_bigdatasetgan.mp4">video</a>
              /
              <a href="https://arxiv.org/abs/2201.04684">arXiv</a>
              <p></p>
              <p>
                We extend DatasetGAN to large-scale dataset ImageNet with as few as 5 annotations per ImageNet category.
              </p>
            </td>
          </tr>
          <tr onmouseout="editgan_stop()" onmouseover="editgan_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='editgan_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/editgan_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/editgan_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function editgan_start() {
                  document.getElementById('editgan_image').style.opacity = "1";
                }
      
                function editgan_stop() {
                  document.getElementById('editgan_image').style.opacity = "0";
                }
                editgan_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://nv-tlabs.github.io/editGAN/">
                <papertitle>EditGAN: High-Precision Semantic Image Editing</papertitle>
              </a>
              <br>
              <a href="http://www.cs.toronto.edu/~linghuan/">Huan Ling</a>,
              <a href="https://karstenkreis.github.io/">Karsten Kreis</a>,
              <strong>Daiqing Li</strong>, <br>
              <a href="https://seung-kim.github.io/seungkim/">Seung Wook Kim</a>,
              <a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba</a>,
              <a href="http://www.cs.toronto.edu/~fidler/">Sanja Fidler</a>
              <br>
              <em>NeurIPS</em>, 2021
              <br>
              <a href="https://nv-tlabs.github.io/editGAN/">project page</a>
              /
              <a href="https://nv-tlabs.github.io/editGAN/resources/demo2.gif">video</a>
              /
              <a href="https://arxiv.org/abs/2111.03186">arXiv</a>
              <p></p>
              <p>
                We use GAN to model joint distribution of images and semantic labels, and use it for semantic aware image editing.
              </p>
            </td>
          </tr>
          <tr onmouseout="seggan_stop()" onmouseover="seggan_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='seggan_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/seggan_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/seggan_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function seggan_start() {
                  document.getElementById('seggan_image').style.opacity = "1";
                }
      
                function seggan_stop() {
                  document.getElementById('seggan_image').style.opacity = "0";
                }
                seggan_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://nv-tlabs.github.io/semanticGAN/">
                <papertitle>Semantic Segmentation with Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization</papertitle>
              </a>
              <br>
              <strong>Daiqing Li</strong>,
              <a href="https://scholar.google.com/citations?user=QYkscc4AAAAJ&hl=en">Junlin Yang</a>,
              <a href="https://karstenkreis.github.io/">Karsten Kreis</a>, <br>
              <a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba</a>,
              <a href="http://www.cs.toronto.edu/~fidler/">Sanja Fidler</a>
              <br>
              <em>CVPR</em>, 2021
              <br>
              <a href="https://nv-tlabs.github.io/semanticGAN/">project page</a>
              /
              <a href="https://twitter.com/i/status/1407448919332765698">video</a>
              /
              <a href="https://arxiv.org/abs/2104.05833">arXiv</a>
              <p></p>
              <p>
                We use generative models to model joint distribution of images and semantic labels, and use it for semi-supervised learning and out-of-domain generalization.
              </p>
            </td>
          </tr>
          <tr onmouseout="fedsim_stop()" onmouseover="fedsim_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='fedsim_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/fedsim_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/fedsim_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function fedsim_start() {
                  document.getElementById('fedsim_image').style.opacity = "1";
                }

                function fedsim_stop() {
                  document.getElementById('fedsim_image').style.opacity = "0";
                }
                fedsim_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://research.nvidia.com/labs/toronto-ai/fed-sim/">
                <papertitle>Federated Simulation for Medical Imaging</papertitle>
              </a>
              <br>
              <strong>Daiqing Li</strong>,
              <a href="http://www.cs.toronto.edu/~amlan/">Amlan Kar</a>,
              <a href="https://eps.leeds.ac.uk/computing/staff/1846/dr-nishant-ravikumar/">Nishant Ravikumar</a>, <br>
              <a href="https://eps.leeds.ac.uk/computing/staff/1535/professor-alejandro-f-frangi/">Alejandro F Frangi</a>,
              <a href="http://www.cs.toronto.edu/~fidler/">Sanja Fidler</a>
              <br>
              <em>MICCAI</em>, 2020 &nbsp <font color="red"><strong>(Young Scientist Awards (YSA) Runner-up)</strong></font>
              <br>
              <a href="https://research.nvidia.com/labs/toronto-ai/fed-sim/">project page</a>
              /
              <a href="https://research.nvidia.com/labs/toronto-ai/fed-sim/resources/demo.mp4">video</a>
              /
              <a href="https://arxiv.org/abs/2009.00668">arXiv</a>
              <p></p>
              <p>
                We introduce a physics-driven generative approach that consists of two learnable neural modules: 1) a module that synthesizes 3D cardiac shapes along with their materials, and 2) a CT simulator that renders these into realistic 3D CT Volumes, with annotations.
              </p>
            </td>
          </tr>

          <tr onmouseout="ntg_stop()" onmouseover="ntg_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ntg_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/ntg_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/ntg_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function ntg_start() {
                  document.getElementById('ntg_image').style.opacity = "1";
                }

                function ntg_stop() {
                  document.getElementById('ntg_image').style.opacity = "0";
                }
                ntg_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://nv-tlabs.github.io/NTG/">
                <papertitle>Neural Turtle Graphics for Modeling City Road Layouts</papertitle>
              </a>
              <br>
              <a href="https://chuhang.github.io/">Hang Chu</a>,
              <strong>Daiqing Li</strong>,
              <a href="http://www.cs.toronto.edu/~davidj/">David Acuna</a>,
              <a href="http://www.cs.toronto.edu/~amlan/">Amlan Kar</a>,
              <a href="http://shumash.com/">Maria Shugrina</a>, <br>
              <a href="http://kyewei.com/">Xinkai Wei</a>,
              <a href="http://mingyuliu.net/">Ming-Yu Liu</a>,
              <a href="http://web.mit.edu/torralba/www/">Antonio Torralba</a>,
              <a href="http://www.cs.toronto.edu/~fidler/">Sanja Fidler</a>
              <br>
              <em>ICCV</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://nv-tlabs.github.io/NTG/">project page</a>
              /
              <a href="https://nv-tlabs.github.io/NTG/resources/ntg_final.mp4">video</a>
              /
              <a href="https://arxiv.org/abs/1910.02055">arXiv</a>
              <p></p>
              <p>
              We propose Neural Turtle Graphics (NTG) to model spatial graphs, demostrate application in city road layouts generation.
              </p>
            </td>
          </tr>

          <tr onmouseout="face2face_stop()" onmouseover="face2face_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='face2face_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/face2face_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/face2face_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function face2face_start() {
                  document.getElementById('face2face_image').style.opacity = "1";
                }

                function face2face_stop() {
                  document.getElementById('face2face_image').style.opacity = "0";
                }
                face2face_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://chuhang.github.io/projects/face2face/index.html">
                <papertitle>A Face-to-Face Neural Conversation Model</papertitle>
              </a>
              <br>
              <a href="https://chuhang.github.io/">Hang Chu</a>,
              <strong>Daiqing Li</strong>,
              <a href="http://www.cs.toronto.edu/~fidler/">Sanja Fidler</a>
              <br>
              <em>CVPR</em>, 2018
              <br>
              <a href="https://chuhang.github.io/projects/face2face/index.html">project page</a>
              /
              <a href="https://vimeo.com/263615590">video</a>
              /
              <a href="https://arxiv.org/abs/1812.01525">arXiv</a>
              <p></p>
              <p>
              We use an RNN encoder-decoder that exploits the movement of facial muscles, as well as the verbal conversation.
              </p>
            </td>
          </tr>


        </tbody></table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tbody>
          <tr>
              <td>
                  <heading>Professional Service</heading>
              </td>
          </tr>
          </tbody>
        </table>

        <table border="0px" width="100%" align="center" border="0" cellpadding="10">
            <tr>
                <td>
                    <strong>Conference Reviewer:</strong> CVPR, ICCV, ECCV, NeurIPS, ICLR
                </td>
            </tr>
        </table>

        <br>
        <br>
				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
          <tr >
           <td style="padding:0px;width:23%;vertical-align:left">
             <br>
             <div style="width: 50%; text-align: center; margin: 0 auto;">
              <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=1IPf0Fo3ktm5IO3crx8N7GXZRlsf8B-tprE0j0O560Y&cl=ffffff&w=a"></script>
             </div>
           </td>
        </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Template from <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. 
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
